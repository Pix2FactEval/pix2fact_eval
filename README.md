<p align="center">
  <img src="assets/logo1_pix2fact.png" alt="Pix2Fact Logo" width="120">
</p>

<p align="center">
        &nbsp&nbspðŸ¤— <a href="https://huggingface.co/datasets/pix2fact/Pix2FactBenchmark">Hugging Face Dataset</a>&nbsp</a>&nbsp&nbsp | &nbsp&nbspðŸ“‘ <a href="">Paper</a>&nbsp&nbsp
<br>


A visual QA benchmark evaluating expert-level perception and knowledge-intensive multi-hop reasoningâ€”SOTA VLMs reach only 24% accuracy vs. 56% human performance.

ä¸€ä¸ªè¯„æµ‹ä¸“å®¶çº§è§†è§‰æ„ŸçŸ¥ä¸ŽçŸ¥è¯†å¯†é›†åž‹å¤šè·³æŽ¨ç†çš„è§†è§‰é—®ç­”åŸºå‡†ï¼›å½“å‰æœ€ä¼˜ VLM å‡†ç¡®çŽ‡ä»… 24%ï¼Œäººç±»è¾¾ 56%ã€‚

> [**From Pixels to Facts (Pix2Fact): Benchmarking Multi-Hop Reasoning for Fine-Grained Visual Fact Checking**]()<br>
> [Yifan Jiang](), [Cong Zhang](), [Bofei Zhang](), [Yifan Yang](), [Bingzhang Wang](), [Yew Soon Ong]()
<p align="center">
  <img src="assets/teaser.png" alt="Pix2Fact Logo" width="100%">
</p>

---

## Getting Started

*Code Coming Soon*

---

## Citation

If you use Pix2Fact in your research, please cite:

```bibtex
@article{jiang2025pix2fact,
  title={From Pixels to Facts (Pix2Fact): Benchmarking Multi-Hop Reasoning for Fine-Grained Visual Fact Checking},
  author={Jiang, Yifan and Zhang, Cong and Zhang, Bofei and Yang, Yifan and Wang, Bingzhang and Ong, Yew Soon},
  journal={},
  year={2025}
}
```

---

## License

*To be specified.*
